function net = Init_LEARN_Model


lr  = [1 1];
weightDecay = [1 0];

% Define network
net.layers = {} ;

 for i = 1:1:50
net.layers{end+1} = struct('type', 'dzdysum');
net.layers{end+1} = struct('type', 'CTmapping','weights',{{0}}, 'learningRate',lr, 'weightDecay',weightDecay);
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{0.01*randn(5,5,1,48,'single'), zeros(48,1,'single')}}, ...
    'stride', 1, ...
    'pad', 2, ...
    'learningRate',lr, ...
    'weightDecay',weightDecay, ...
    'opts',{{}}) ;
net.layers{end+1} = struct('type', 'relu','leak',0) ;
net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{0.01*randn(5,5,48,48,'single'), zeros(48,1,'single')}}, ...
    'stride', 1, ...
    'pad', 2, ...
    'learningRate',lr, ...
    'weightDecay',weightDecay, ...
    'opts',{{}}) ;
net.layers{end+1} = struct('type', 'relu','leak',0) ;

net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{0.01*randn(5,5,48,1,'single'), zeros(1,1,'single')}}, ...
    'stride', 1, ...
    'pad', 2, ...
    'learningRate',lr, ...
    'weightDecay',weightDecay, ...
    'opts',{{}}) ;

net.layers{end+1} = struct('type', 'sum');
net.layers{end+1} = struct('type', 'relu','leak',0) ;
 end

net.layers{end+1} = struct('type', 'loss') ; % make sure the new 'vl_nnloss.m' is in the same folder.
net = vl_simplenn_tidy(net);